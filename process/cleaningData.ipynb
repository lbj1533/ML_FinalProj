{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87e318a-7a80-48a8-842f-9cfe48120d7f",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e04682-4665-4165-9889-7abeebf7a0b2",
   "metadata": {},
   "source": [
    "Below describes our process for cleaning the data, removing unwanted features, adding features, and normalizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877f336-12b6-4e94-ba94-b2cb9ead2162",
   "metadata": {},
   "source": [
    "## Dropping Unimportant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60130a42-c228-4012-b0d6-a1302776a5ef",
   "metadata": {},
   "source": [
    "This section describes which features from the original data we remove and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0256caf2-9f32-4727-b337-66ec27ca450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f71c0d-6c73-4ec4-8e61-d85767c67943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading in data\n",
    "\n",
    "data_folder = \"../data/raw/\"\n",
    "file = data_folder + \"all_songs_data_raw.csv\"\n",
    "\n",
    "# concat all csvs into one dataframe\n",
    "df_list = [pd.read_csv(file)]\n",
    "\n",
    "raw_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f25d5a-1652-4d3d-83ec-918403e44291",
   "metadata": {},
   "source": [
    "We will start by removing any features that will not be useful due to their form, like \"Media\" or \"Writers\". These are URLs or other embedded information, and are not useful or interesting to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ed53ae-1c9a-485b-bed1-09107d84968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying soon-to-be-cleaned data\n",
    "cleaned_df = raw_df.copy()\n",
    "\n",
    "# dropping writers and media columns\n",
    "cleaned_df = cleaned_df.drop(columns=[\"Media\", \"Writers\"])\n",
    "\n",
    "# also drop album and song urls\n",
    "cleaned_df = cleaned_df.drop(columns=[\"Album URL\", \"Song URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d1d41-85ec-4b86-9b4f-a1d7bc5093ae",
   "metadata": {},
   "source": [
    "We will then drop features that are simply redundant, like \"Date\" when all we care about is predicting the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402b011d-13c0-48df-978d-3021d4254a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.drop(columns=[\"Release Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e4518-d6c0-4ee9-acd7-a10e4ef53feb",
   "metadata": {},
   "source": [
    "## Cleaning Data Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d8b58-2a75-4bf0-b132-83fce66dfb45",
   "metadata": {},
   "source": [
    "This section will describe our process for cleaning up values in the data.\n",
    "\n",
    "The first values we will clean are all the values in the \"Year\" column. They are currently floats, when they can easily be ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c405482-e73d-4646-9247-31496c20615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df[\"Year\"] = cleaned_df[\"Year\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fc4d4-a150-4ca2-9762-5d7e1d98fb86",
   "metadata": {},
   "source": [
    "Next we will clean the \"Featured Artists\" column, as right now it is comprised of metadata, when we want the names of the artists only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98cb65d-1260-4704-a8bd-ddafeb285404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "combined_artists_list = []\n",
    "\n",
    "for i, row in cleaned_df.iterrows():\n",
    "    artist = row[\"Artist\"]\n",
    "    featured_artists = row[\"Featured Artists\"]\n",
    "\n",
    "    # ensure featured artists is a string of a list of dicts\n",
    "    if isinstance(featured_artists, str) and featured_artists != \"[]\":\n",
    "        # convert if needed\n",
    "        featured_artists = ast.literal_eval(featured_artists)\n",
    "\n",
    "    if isinstance(featured_artists, list) and featured_artists:\n",
    "\n",
    "        # get all names\n",
    "        featured_artists_names = [fa['name'] for fa in featured_artists if isinstance(fa, dict) and 'name' in fa]\n",
    "\n",
    "        # combine names if they dont match\n",
    "        combined_artists = [artist]\n",
    "        for fa_name in featured_artists_names:\n",
    "            if fa_name.lower() not in artist.lower():\n",
    "                combined_artists.append(fa_name)\n",
    "                \n",
    "        # deals with adding to csv\n",
    "        combined_artists_list.append(\", \".join(combined_artists))\n",
    "    else:\n",
    "        combined_artists_list.append(artist)\n",
    "        \n",
    "cleaned_df[\"Artists\"] = combined_artists_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1778f-3a31-4045-b1c2-508304eba0cf",
   "metadata": {},
   "source": [
    "There is still one more thing to be done with this column, though. Many bands are named \"[Main Vocalist] and the [Band Name]\", especially in the earlier years of this dataset. This should be treated as one artist, if possible. This is different from many songs today that have 2 or more artists separated by \"and\" in the format \"[1st Artist] and [2nd Artist]\". There needs to be a way to distinguish these two, as this may affect the model.\n",
    "\n",
    "A note: after many attempts at separating out these two cases, we have to concede that this will very likely have to be done manually or through a method that we do not have the time to persue right now. Unfortunately, we will have to accept that the model will have to use this flawed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172817d3-1013-41b3-aee9-cd22b34a152c",
   "metadata": {},
   "source": [
    "Now that \"Artists\" is a column and is finalized, there is no need for the Artists and Featured Artist columns, so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeeb3ea4-0210-4a78-bf7a-964af10d2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.drop(columns=[\"Artist\", \"Featured Artists\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac882091-f324-455b-9674-147cca6bb1e1",
   "metadata": {},
   "source": [
    "## Removing Non-Applicable Data Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486e459-9860-4834-8f09-6c395c6394e0",
   "metadata": {},
   "source": [
    "Now we will remove any instrumental songs. The \"lyrics\" are unpredictable in their representation and we cannot do sentiment analysis on them, so they must be removed. Oddly enough, there is no definite way to do this, but from looking at the data, removing any row with an empty lyrics, adverb, nouns, corpus or verbs column will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876aa008-34af-4018-a011-d807ec06c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.dropna(subset=[\"Lyrics\", \"Verbs\", \"Nouns\", \"Adverbs\", \"Corpus\"], how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3d80e-bfca-4dae-8faa-6f595f0748b7",
   "metadata": {},
   "source": [
    "As you can see, all of the songs with the smallest number of lyrics have actual words instead of symbols or placeholders that would signify an instrumental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b16299-f4f9-438d-9c97-e85ddd85a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Lyrics  Word Counts\n",
      "5952  I love it when you call me A-nita cause it's m...           19\n",
      "1504  Spoken: Grand piano Reed and pipe organ Glocke...           26\n",
      "1849  Guess mine is not the first heart broken My ey...           28\n",
      "23    Sorry, sorry, oh so sorry SPOKEN: Uh-oh! **I r...           33\n",
      "1686  Baby face, youve got the cutest little baby fa...           55\n",
      "340   (Roy Orbison)  Sweet dream baby Sweet dream ba...           62\n",
      "845   Here he comes now I've got to tell him somehow...           62\n",
      "1318  DAY BY DAY GODSPELL Day by day (solo voice) Da...           65\n",
      "1007  This brand new album is called Hawaii Five-O P...           66\n",
      "2492   The dark side's callin' now Nothin' is real S...           66\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df.nsmallest(10, 'Word Counts')[['Lyrics', 'Word Counts']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad5d82-0c6c-402c-a5e1-afe3424ffe94",
   "metadata": {},
   "source": [
    "## Creating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607c546-7fcb-4317-8717-129bacb5f884",
   "metadata": {},
   "source": [
    "The below code creates a new feature that is the ratio between the total amount of words and the unique words, thus creating a \"Repetition Ratio\". For example, if a song has 180 words and the amount of unique words is 90, the repetition ratio is 2, meaning each word is said twice on average.\n",
    "\n",
    "Note: This feature ended up not being helpful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9e1bee-0359-4c2c-8d5a-ee62bb0b44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_df[\"Repetition_Ratio\"] = cleaned_df[\"Word Counts\"] / cleaned_df[\"Unique Word Counts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12675bd-1cc7-4d4d-9c3d-b84754208244",
   "metadata": {},
   "source": [
    "The next feature we can create is the sentiment score for the corpus of the lyrics. In theory this method could be applied to the verbs, adverbs, nouns, etc. of the song, but this is unlikely to provide useful results. However, the model could use the sentiment score of the lyrics of a song to help predict its year, if there is a pattern.\n",
    "\n",
    "The following code will use Natural Language Toolkit for sentiment analysis on these lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c4e5932-4769-4d11-a70a-e8f704ab3adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/devel/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk import download\n",
    "\n",
    "# download lexicons\n",
    "download('opinion_lexicon')\n",
    "\n",
    "# set of pos and neg words\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "# funct to apply to each song\n",
    "def lexicon_sentiment_score(tokens):\n",
    "\n",
    "    if not isinstance(tokens, list): \n",
    "        return 0.5\n",
    "    \n",
    "    # get pos and neg count\n",
    "    pos_count = sum(token in positive_words for token in tokens)\n",
    "    neg_count = sum(token in negative_words for token in tokens)\n",
    "    total = pos_count + neg_count\n",
    "\n",
    "    # return ratio aka sentiment score\n",
    "    return 0.5 if total == 0 else pos_count / total\n",
    "\n",
    "cleaned_df[\"Tokens\"] = cleaned_df[\"Corpus\"].str.lower().str.split() #pretokenize text\n",
    "cleaned_df[\"Lyrics_Sentiment\"] = cleaned_df[\"Tokens\"].apply(lexicon_sentiment_score)  # apply sentiment analysis\n",
    "cleaned_df = cleaned_df.drop(columns=\"Tokens\", axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa5973-262a-429c-af54-5bcd2877e795",
   "metadata": {},
   "source": [
    "Upon further analysis, it has been determined that more features would be helpful. We will apply the same sentiment analysis on the album names and the song names.\n",
    "\n",
    "Note: These features ended up not being helpful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6c973e-c51d-4cdf-8b34-71be45dcc458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncleaned_df[\"Tokens\"] = cleaned_df[\"Album\"].str.lower().str.split()\\ncleaned_df[\"Album_Sentiment\"] = cleaned_df[\"Tokens\"].apply(lexicon_sentiment_score)\\ncleaned_df = cleaned_df.drop(columns=\"Tokens\", axis=1)\\n\\ncleaned_df[\"Tokens\"] = cleaned_df[\"Song Title\"].str.lower().str.split()\\ncleaned_df[\"Song_Sentiment\"] = cleaned_df[\"Tokens\"].apply(lexicon_sentiment_score)\\ncleaned_df = cleaned_df.drop(columns=\"Tokens\", axis=1)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cleaned_df[\"Tokens\"] = cleaned_df[\"Album\"].str.lower().str.split()\n",
    "cleaned_df[\"Album_Sentiment\"] = cleaned_df[\"Tokens\"].apply(lexicon_sentiment_score)\n",
    "cleaned_df = cleaned_df.drop(columns=\"Tokens\", axis=1)\n",
    "\n",
    "cleaned_df[\"Tokens\"] = cleaned_df[\"Song Title\"].str.lower().str.split()\n",
    "cleaned_df[\"Song_Sentiment\"] = cleaned_df[\"Tokens\"].apply(lexicon_sentiment_score)\n",
    "cleaned_df = cleaned_df.drop(columns=\"Tokens\", axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a6c2c-867c-4453-8e4f-9a45552d62ac",
   "metadata": {},
   "source": [
    "The following code will calculate the average number of words per sentence in the lyrics. This and the following features will be created so the model will have more to work from.\n",
    "\n",
    "Note: This feature ended up not being helpful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c698643-d67a-4da5-8bdf-5d55ea7030bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def average_sentence_length(text):\n",
    "    # split into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    # count total words\n",
    "    total_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "    # avg sentence length\n",
    "    return total_words / len(sentences) if sentences else 0\n",
    "\n",
    "#cleaned_df['Avg_Sentence_Length'] = cleaned_df['Lyrics'].apply(average_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13de26-0e1a-4282-9b6b-504951b1e3d8",
   "metadata": {},
   "source": [
    "The average word length may also be a helpful feature. The following code creates this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0bcbeb6-7c4e-4bd9-94fd-3ede2d24bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(lyrics):\n",
    "    words = lyrics.split()\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    return total_length / len(words) if words else 0\n",
    "\n",
    "cleaned_df['Avg_Word_Length'] = cleaned_df['Lyrics'].apply(average_word_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c5420-e9f1-4139-bb29-0c3a50d61e66",
   "metadata": {},
   "source": [
    "We will add the Flesch-Kincaid Readability formula, again to give the model more to work with.\n",
    "\n",
    "Note: This feature ended up not being helpful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f85bd0ba-1fbb-404c-a6e5-024127001ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability_score(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "    \n",
    "#cleaned_df['Readability_Score'] = cleaned_df['Lyrics'].apply(calculate_readability_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8409479-dc90-4e67-bd52-4c641b25b795",
   "metadata": {},
   "source": [
    "We will also add number of verbs, nouns and adverbs. Note: This section has been added after noting feature importance with a Random Forest Model. It is apparent that the only standout feature is Word Count. We hope that adding more similar features, like the number of verbs, nouns and adverbs would be helpful to the model. Another Note: This seemed to have a positive impact, however, we believe that more features of a similar nature could help the model.\n",
    "\n",
    "Final Note: These features were ultimately not helpful to the model. They have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186df811-b53e-49d3-8613-e72325018de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cleaned_df['Verb_Count'] = cleaned_df['Verbs'].apply(lambda x: len(x.split()))\n",
    "#cleaned_df['Noun_Count'] = cleaned_df['Nouns'].apply(lambda x: len(x.split()))\n",
    "#cleaned_df['Adverb_Count'] = cleaned_df['Adverbs'].apply(lambda x: len(x.split()))\n",
    "\n",
    "#cleaned_df['Verb to Noun'] = cleaned_df['Verb_Count'] / cleaned_df['Noun_Count']\n",
    "#cleaned_df['Noun to Adverb'] = cleaned_df['Noun_Count'] / cleaned_df['Adverb_Count']\n",
    "#cleaned_df['Adverb to Verb'] = cleaned_df['Adverb_Count'] / cleaned_df['Verb_Count']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20f517-0ebb-4e72-a006-7d9ad62411c7",
   "metadata": {},
   "source": [
    "## Standardizing Non-Numeric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b25d8a-2e3e-4b9d-935b-665024f96175",
   "metadata": {},
   "source": [
    "The following will contain code on standardizing non-numeric features, so the model, again, has more to work with. We will use label encoding because of the memory efficiency in avoiding the extremely high dimensionality of all of the unique artists. This has its drawbacks, but we want to give the model all of the data it needs to find a pattern.\n",
    "\n",
    "The features we will label encode are Album and Artists. We will also scale them to be between 0 and 1 with a Standard Deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168d0704-ab08-4bea-b649-4b6c302471cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "cleaned_df['Album_Encoded'] = label_encoder.fit_transform(cleaned_df['Album'])\n",
    "cleaned_df['Artists_Encoded'] = label_encoder.fit_transform(cleaned_df['Artists'])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cleaned_df['Artists_Normalized'] = scaler.fit_transform(cleaned_df[['Artists_Encoded']])\n",
    "cleaned_df['Album_Normalized'] = scaler.fit_transform(cleaned_df[['Album_Encoded']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df69dd-d634-4ab2-b06d-19a8decc36db",
   "metadata": {},
   "source": [
    "## Standardizing Numeric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e73c20-9c78-424b-97a6-e7eeb14f7be7",
   "metadata": {},
   "source": [
    "Rank needs to be standardized between 0 and 1, where lower ranks are better. Note: This feature ended up not being helpful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5fa05e1-33ce-4b7e-b8ac-508de878cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#cleaned_df['Rank'] = scaler.fit_transform(cleaned_df[['Rank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75d80e-c0f7-42e2-84cf-72368ff03a1b",
   "metadata": {},
   "source": [
    "We will continue to standardize features, now focusing on numerical features. We will standardize these to have a mean of 0, and a standard deviation of 1 for best results. Thus, we will use StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc4409dd-2b60-4d37-9363-c374afc513e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original features\n",
    "'''\n",
    "numerical_features = ['Word Counts', 'Unique Word Counts', 'Repetition_Ratio', \n",
    "                      'Lyrics_Sentiment', 'Song_Sentiment', 'Album_Sentiment',\n",
    "                      'Avg_Sentence_Length', 'Avg_Word_Length', 'Readability_Score', \n",
    "                      'Verb_Count', 'Adverb_Count', 'Noun_Count',\n",
    "                      'Verb to Noun', 'Noun to Adverb', 'Adverb to Verb'\n",
    "                     ]\n",
    "'''\n",
    "# trimmed features to be important\n",
    "numerical_features = ['Word Counts', 'Lyrics_Sentiment', 'Avg_Word_Length', 'Artists_Normalized', 'Album_Normalized']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cleaned_df[numerical_features] = scaler.fit_transform(cleaned_df[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ae1ee-cabe-4a96-9b1a-24eda0c10070",
   "metadata": {},
   "source": [
    "## Final Dropping of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6970f-a696-4aa0-813d-4e786030d4e7",
   "metadata": {},
   "source": [
    "We will now drop the features we have used to extract numerical and standardized features from. They can no longer be used by us, and the model cannot use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b90b40-d545-4558-827f-3d05c1554e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"Album\", \"Lyrics\", \"Rank\", \"Song Title\", \n",
    "                   \"Verbs\", \"Nouns\", \"Adverbs\", \"Corpus\", \n",
    "                   \"Artists\", \"Album_Encoded\", \"Artists_Encoded\"]\n",
    "cleaned_df = cleaned_df.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9031fb0-6cb7-4752-9bc2-aa65fc4c943e",
   "metadata": {},
   "source": [
    "## Saving the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe71f8-f89c-4d23-80fb-c9d48b8897d8",
   "metadata": {},
   "source": [
    "Finally we save the cleaned data to its new location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "356d3c45-0024-468a-a972-06b71f836648",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_folder = \"../data/cleaned/\"\n",
    "cleaned_file = cleaned_data_folder + \"all_songs_data_cleaned.csv\"\n",
    "os.makedirs(cleaned_data_folder, exist_ok=True)\n",
    "cleaned_df.to_csv(cleaned_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
