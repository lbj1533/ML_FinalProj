{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87e318a-7a80-48a8-842f-9cfe48120d7f",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e04682-4665-4165-9889-7abeebf7a0b2",
   "metadata": {},
   "source": [
    "Below describes our process for cleaning the data, removing unwanted features, and other small considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877f336-12b6-4e94-ba94-b2cb9ead2162",
   "metadata": {},
   "source": [
    "## Dropping Unimportant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60130a42-c228-4012-b0d6-a1302776a5ef",
   "metadata": {},
   "source": [
    "This section describes which features from the original data we remove and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0256caf2-9f32-4727-b337-66ec27ca450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f71c0d-6c73-4ec4-8e61-d85767c67943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading in data\n",
    "\n",
    "data_folder = \"../data/raw/\"\n",
    "file = data_folder + \"all_songs_data_raw.csv\"\n",
    "\n",
    "# concat all csvs into one dataframe\n",
    "df_list = [pd.read_csv(file)]\n",
    "\n",
    "raw_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f25d5a-1652-4d3d-83ec-918403e44291",
   "metadata": {},
   "source": [
    "We will start by removing any features that will not be useful due to their form, like \"Media\" or \"Writers\". These are URLs or other embedded information, and are not useful or interesting to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ed53ae-1c9a-485b-bed1-09107d84968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying soon-to-be-cleaned data\n",
    "cleaned_df = raw_df.copy()\n",
    "\n",
    "# dropping writers and media columns\n",
    "cleaned_df = cleaned_df.drop(columns=[\"Media\", \"Writers\"])\n",
    "\n",
    "# also drop album and song urls\n",
    "cleaned_df = cleaned_df.drop(columns=[\"Album URL\", \"Song URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d1d41-85ec-4b86-9b4f-a1d7bc5093ae",
   "metadata": {},
   "source": [
    "We will then drop features that are simply redundant, like \"Date\" when all we care about is predicting the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402b011d-13c0-48df-978d-3021d4254a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.drop(columns=[\"Release Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e4518-d6c0-4ee9-acd7-a10e4ef53feb",
   "metadata": {},
   "source": [
    "## Cleaning Data Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d8b58-2a75-4bf0-b132-83fce66dfb45",
   "metadata": {},
   "source": [
    "This section will describe our process for cleaning up values in the data.\n",
    "\n",
    "The first values we will clean are all the values in the \"Year\" column. They are currently floats, when they can easily be ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c405482-e73d-4646-9247-31496c20615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df[\"Year\"] = cleaned_df[\"Year\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fc4d4-a150-4ca2-9762-5d7e1d98fb86",
   "metadata": {},
   "source": [
    "Next we will clean the \"Featured Artists\" column, as right now it is comprised of metadata, when we want the names of the artists only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98cb65d-1260-4704-a8bd-ddafeb285404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "combined_artists_list = []\n",
    "\n",
    "for i, row in cleaned_df.iterrows():\n",
    "    artist = row[\"Artist\"]\n",
    "    featured_artists = row[\"Featured Artists\"]\n",
    "\n",
    "    # ensure featured artists is a string of a list of dicts\n",
    "    if isinstance(featured_artists, str) and featured_artists != \"[]\":\n",
    "        # convert if needed\n",
    "        featured_artists = ast.literal_eval(featured_artists)\n",
    "\n",
    "    if isinstance(featured_artists, list) and featured_artists:\n",
    "\n",
    "        # get all names\n",
    "        featured_artists_names = [fa['name'] for fa in featured_artists if isinstance(fa, dict) and 'name' in fa]\n",
    "\n",
    "        # combine names if they dont match\n",
    "        combined_artists = [artist]\n",
    "        for fa_name in featured_artists_names:\n",
    "            if fa_name.lower() not in artist.lower():\n",
    "                combined_artists.append(fa_name)\n",
    "                \n",
    "        # deals with adding to csv\n",
    "        combined_artists_list.append(\", \".join(combined_artists))\n",
    "    else:\n",
    "        combined_artists_list.append(artist)\n",
    "        \n",
    "cleaned_df[\"Artists\"] = combined_artists_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1778f-3a31-4045-b1c2-508304eba0cf",
   "metadata": {},
   "source": [
    "There is still one more thing to be done with this column, though. Many bands are named \"[Main Vocalist] and the [Band Name]\", especially in the earlier years of this dataset. This should be treated as one artist, if possible. This is different from many songs today that have 2 or more artists separated by \"and\" in the format \"[1st Artist] and [2nd Artist]\". There needs to be a way to distinguish these two, as this may affect the model.\n",
    "\n",
    "A note: after many attempts at separating out these two cases, we have to concede that this will very likely have to be done manually or through a method that we do not have the time to persue right now. Unfortunately, we will have to accept that the model will have to use this flawed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172817d3-1013-41b3-aee9-cd22b34a152c",
   "metadata": {},
   "source": [
    "Now that \"Artists\" is a column and is finalized, there is no need for the Artists and Featured Artist columns, so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeeb3ea4-0210-4a78-bf7a-964af10d2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.drop(columns=[\"Artist\", \"Featured Artists\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac882091-f324-455b-9674-147cca6bb1e1",
   "metadata": {},
   "source": [
    "## Removing Non-Applicable Data Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486e459-9860-4834-8f09-6c395c6394e0",
   "metadata": {},
   "source": [
    "Now we will remove any instrumental songs. The \"lyrics\" are unpredictable in their representation and we cannot do sentiment analysis on them, so they must be removed. Oddly enough, there is no definite way to do this, but from looking at the data, removing any row with an empty lyrics, adverb, nouns, corpus or verbs column will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876aa008-34af-4018-a011-d807ec06c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.dropna(subset=[\"Lyrics\", \"Verbs\", \"Nouns\", \"Adverbs\", \"Corpus\"], how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3d80e-bfca-4dae-8faa-6f595f0748b7",
   "metadata": {},
   "source": [
    "As you can see, all of the songs with the smallest number of lyrics have actual words instead of symbols or placeholders that would signify an instrumental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b16299-f4f9-438d-9c97-e85ddd85a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Lyrics  Word Counts\n",
      "5952  I love it when you call me A-nita cause it's m...           19\n",
      "1504  Spoken: Grand piano Reed and pipe organ Glocke...           26\n",
      "1849  Guess mine is not the first heart broken My ey...           28\n",
      "23    Sorry, sorry, oh so sorry SPOKEN: Uh-oh! **I r...           33\n",
      "1686  Baby face, youve got the cutest little baby fa...           55\n",
      "340   (Roy Orbison)  Sweet dream baby Sweet dream ba...           62\n",
      "845   Here he comes now I've got to tell him somehow...           62\n",
      "1318  DAY BY DAY GODSPELL Day by day (solo voice) Da...           65\n",
      "1007  This brand new album is called Hawaii Five-O P...           66\n",
      "2492   The dark side's callin' now Nothin' is real S...           66\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df.nsmallest(10, 'Word Counts')[['Lyrics', 'Word Counts']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad5d82-0c6c-402c-a5e1-afe3424ffe94",
   "metadata": {},
   "source": [
    "## Creating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607c546-7fcb-4317-8717-129bacb5f884",
   "metadata": {},
   "source": [
    "The below code creates a new feature that is the ratio between the total amount of words and the unique words, thus creating a \"Repetition Ratio\". For example, if a song has 180 words and the amount of unique words is 90, the repetition ratio is 2, meaning each word is said twice on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9e1bee-0359-4c2c-8d5a-ee62bb0b44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df[\"Repetition Ratio\"] = cleaned_df[\"Word Counts\"] / cleaned_df[\"Unique Word Counts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12675bd-1cc7-4d4d-9c3d-b84754208244",
   "metadata": {},
   "source": [
    "The next feature we can create is the sentiment score for the corpous of the lyrics. In theory this method could be applied to the verbs, adverbs, nouns, etc. of the song, but this is unlikely to provide useful results. However, the model could use the sentiment score of the lyrics of a song to help predict its year, if there is a pattern.\n",
    "\n",
    "The following code will use Natural Language Toolkit for sentiment analysis on these lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c4e5932-4769-4d11-a70a-e8f704ab3adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/devel/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/devel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "\n",
    "# download lexicons\n",
    "download('opinion_lexicon')\n",
    "download('punkt')\n",
    "\n",
    "# funct to apply to each song\n",
    "def lexicon_sentiment_score(text):\n",
    "    \n",
    "    # set of pos and neg words\n",
    "    positive_words = set(opinion_lexicon.positive())\n",
    "    negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "    # normalize corpuses\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # get pos and neg count\n",
    "    pos_count = sum(1 for word in words if word in positive_words)\n",
    "    neg_count = sum(1 for word in words if word in negative_words)\n",
    "    total = pos_count + neg_count\n",
    "\n",
    "    # return ratio aka sentiment score\n",
    "    if total == 0:\n",
    "        return 0.5\n",
    "    return pos_count / total\n",
    "\n",
    "cleaned_df[\"Sentiment\"] = cleaned_df[\"Corpus\"].apply(lexicon_sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9031fb0-6cb7-4752-9bc2-aa65fc4c943e",
   "metadata": {},
   "source": [
    "## Saving the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe71f8-f89c-4d23-80fb-c9d48b8897d8",
   "metadata": {},
   "source": [
    "Finally we save the cleaned data to its new location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356d3c45-0024-468a-a972-06b71f836648",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_folder = \"../data/cleaned/\"\n",
    "cleaned_file = cleaned_data_folder + \"all_songs_data_cleaned.csv\"\n",
    "os.makedirs(cleaned_data_folder, exist_ok=True)\n",
    "cleaned_df.to_csv(cleaned_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
